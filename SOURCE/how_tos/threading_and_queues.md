# 线程和队列 <a class="md-anchor" id="AUTOGENERATED-threading-and-queues"></a>
在TensorFlow进行异步计算中，队列是一种强大的机制。

在TensorFlow中，一个队列就是TensorFlow图中的一个节点。这是一种有状态的节点，就像变量一样：其他节点可以修改它的内容。具体来说，节点可以把新元素放在一个队列中，也可以把队列中的一个存在的元素去掉。

为了感受一下队列，让我们看一个简单的例子。我们先创建一个“先入先出”的队列（FIFOQueue），并用零填充。然后，我们将构建一个TensorFlow图，从队列中拿走一个元素，添加另外一个到该元素，并把它放回队列的末尾。慢慢地，队列的元素就会增加。

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="IncremeterFifoQueue.gif">
</div>
`Enqueue`、 `EnqueueMany`和`Dequeue`都是特殊的节点。他们需要一个指针指向队列，而不是一个正常的值，允许他们去改变指向的队列。我们建议您将它们看作是类似队列的方法。事实上，在Python API中，它们就是队列对象的方法（例如`q.enqueue(...)`）。

现在你已经对队列有了一定的了解，让我们深入到细节...

## 队列使用概述 <a class="md-anchor" id="AUTOGENERATED-queue-use-overview"></a>
队列，如`FIFOQueue`和`RandomShuffleQueue`，在TensorFlow的张量异步计算时是非常重要的TensorFlow对象。

例如，一个典型的输入结构：是使用一个`RandomShuffleQueue`来作为训练模型的输入：

* 多个线程准备训练样本，并且把这些样本放在队列中。
* 一个训练线程执行一个训练操作，将队列划分为最小的批量。

这种结构具有许多优点，如在[Reading data how to](../reading_data)中强调的，同时，[Reading data how to]也给出了一种概括的功能来简化输入管道的构造。

TensorFlow的`Session`对象是多线程的，因此多个线程可以很方便地使用同一个会话而且并行的执行操作。然而，要实现启动线程的Python程序也不是很容易的。所有线程都必须能够同时停止，必须能够捕获并报告异常，同时队列必须正确的关闭。

TensorFlow提供了两个类来实现：[tf.Coordinator](../../api_docs/python/train.md#Coordinator)和
[tf.QueueRunner](../../api_docs/python/train.md#QueueRunner)。这两个类被设计成一起使用。`Coordinator`类可以用来同时停止多线程而且给一个一直等待所有线程停止的程序报告异常。`QueueRunner`类用来保证多个线程共同将张量在放同一个队列中。

## Coordinator <a class="md-anchor" id="AUTOGENERATED-coordinator"></a>
Coordinator类有助于同步多个线程，使得多个线程同时停止。
其主要方法有：
* `should_stop()`:如果线程应该停止返回True。
* `request_stop(<exception>)`: 请求该线程停止。
* `join(<list of threads>)`:等待一直到指定的线程都停止了。

首先创建一个`Coordinator`对象，然后建立一些线程，使用线程同步。这些线程通常一直循环运行，一直到`should_stop()`返回True时会同时停止。
任何线程都可以决定计算什么时候应该停止。它只需要调用`request_stop()`，同时其他线程的`should_stop()`将会返回`True`，然后都停下来。

```python
# 线程体：循环执行，直到同步机制标识收到了停止请求。
# 如果某些条件为真，请求同步机制去停止线程。
def MyLoop(coord):
  while not coord.should_stop():
    ...do something...
    if ...some condition...:
      coord.request_stop()

# Main code: create a coordinator.
coord = Coordinator()

# Create 10 threads that run 'MyLoop()'
threads = [threading.Thread(target=MyLoop, args=(coord)) for i in xrange(10)]

# Start the threads and wait for all of them to stop.
for t in threads: t.start()
coord.join(threads)
```
显然，Coordinator可以管理线程去做不同的事情。上面的代码只是举例，不必跟上面的例子完全相同。Coordinator还支持捕捉和报告异常。可以参考[Coordinator class](../../api_docs/python/train.md#Coordinator)文档的更多细节。

## QueueRunner <a class="md-anchor" id="AUTOGENERATED-queuerunner"></a>
`QueueRunner`类创建了一些重复运行排队操作的线程。这些线程可以同步一起停止。此外，一个queue runner会运行一个*closer thread*，这个线程在遇到异常时会把异常报告给同步器，然后自动关闭队列。

您可以使用一个queue runner，来实现上述结构。
首先建立一个TensorFlow图表，这个图表使用队列作为输入样本。Add操作会处理这些样本并且把他们添加到队列中。增加training操作来启动从队列将这些样本移除队列的操作。

```python
example = ...ops to create one example...
# Create a queue, and an op that enqueues examples one at a time in the queue.
queue = tf.RandomShuffleQueue(...)
enqueue_op = queue.enqueue(example)
# Create a training graph that starts by dequeuing a batch of examples.
inputs = queue.dequeue_many(batch_size)
train_op = ...use 'inputs' to build the training part of the graph...
```
在Python的训练程序中，创建一个`QueueRunner`将运行几个线程来处理和将样本排成队列。创建一个`Coordinator`，让queue runner启动它的线程，这些线程有同步器进行连接。写一个带同步器的训练循环。

```
# Create a queue runner that will run 4 threads in parallel to enqueue
# examples.
qr = tf.train.QueueRunner(queue, [enqueue_op] * 4)

# Launch the graph.
sess = tf.Session()
# Create a coordinator, launch the queue runner threads.
coord = tf.train.Coordinator()
enqueue_threads = qr.create_threads(sess, coord=coord, start=True)
# Run the training loop, controlling termination with the coordinator.
for step in xrange(1000000):
    if coord.should_stop():
        break
    sess.run(train_op)
# When done, ask the threads to stop.
coord.request_stop()
# And wait for them to actually do it.
coord.join(threads)
```
## 异常处理 <a class="md-anchor" id="AUTOGENERATED-handling-exceptions"></a>
通过queue runners启动的线程不仅仅只做排队操作。他们还捕捉和处理由队列产生的异常，包括`OutOfRangeError`异常，这个异常是用于报告队列被关闭了。
使用同步的训练程序在主循环中捕捉和报告异常时必须相同。
A training program that uses a coordinator must similarly catch and report
exceptions in its main loop.
下面是对上面训练循环的改进版本。

```python
try:
    for step in xrange(1000000):
        if coord.should_stop():
            break
        sess.run(train_op)
except Exception, e:
   # Report exceptions to the coordinator.
   coord.request_stop(e)

# Terminate as usual.  It is innocuous to request stop twice.
coord.request_stop()
coord.join(threads)
